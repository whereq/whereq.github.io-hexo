---
title: Faiss practices
date: 2025-05-05 22:34:40
categories:
- Faiss
- Vectorizer
tags:
- Faiss
- Vectorizer
---

### **ä¸€ã€ç³»ç»Ÿæ¶æ„è®¾è®¡**
```mermaid
sequenceDiagram
    participant User
    participant QueryPlatform
    participant ChatGPT
    participant FAISS
    participant RDBMS
    
    User->>QueryPlatform: è¾“å…¥è‡ªç„¶è¯­è¨€æŸ¥è¯¢ï¼ˆå¦‚"2024å¹´ç¬¬ä¸‰å­£åº¦ä¹é«˜ç§¯æœ¨é”€å”®ç»Ÿè®¡"ï¼‰
    QueryPlatform->>ChatGPT: å‘é€schemaæ£€ç´¢è¯·æ±‚
    ChatGPT->>FAISS: å‘é‡ç›¸ä¼¼åº¦æœç´¢
    FAISS-->>ChatGPT: è¿”å›ç›¸å…³è¡¨ç»“æ„
    ChatGPT-->>QueryPlatform: ç”ŸæˆSQLæŸ¥è¯¢å»ºè®®
    QueryPlatform->>RDBMS: æ‰§è¡Œæœ€ç»ˆSQL
    RDBMS-->>QueryPlatform: è¿”å›æŸ¥è¯¢ç»“æœ
    QueryPlatform-->>User: å±•ç¤ºç»“æœ
```

---

### **äºŒã€è¯¦ç»†å®ç°æ­¥éª¤**

#### **æ­¥éª¤1ï¼šå‡†å¤‡è¡¨ç»“æ„æ•°æ®**
å‡è®¾å·²æœ‰`database_schema.json`ï¼Œæ ¼å¼å¦‚ä¸‹ï¼š
```json
{
  "tables": [
    {
      "name": "sales",
      "columns": [
        {"name": "id", "type": "int", "description": "é”€å”®è®°å½•ID"},
        {"name": "product_id", "type": "int", "description": "å…³è”productsè¡¨"},
        {"name": "sale_date", "type": "date", "description": "é”€å”®æ—¥æœŸ"},
        {"name": "quantity", "type": "int", "description": "é”€å”®æ•°é‡"}
      ],
      "description": "å­˜å‚¨æ‰€æœ‰é”€å”®è®°å½•"
    },
    {
      "name": "products",
      "columns": [
        {"name": "id", "type": "int", "description": "äº§å“ID"},
        {"name": "name", "type": "varchar(255)", "description": "äº§å“åç§°"},
        {"name": "category", "type": "varchar(100)", "description": "äº§å“ç±»åˆ«"}
      ],
      "description": "å­˜å‚¨äº§å“ä¸»æ•°æ®"
    }
  ]
}
```

#### **æ­¥éª¤2ï¼šå‘é‡åŒ–å­˜å‚¨è¡¨ç»“æ„**
```python
import json
import numpy as np
import faiss
from openai import OpenAI
from tenacity import retry, stop_after_attempt, wait_exponential

# åˆå§‹åŒ–
client = OpenAI(api_key="your_api_key")
index = None  # FAISSç´¢å¼•
schema_data = []  # åŸå§‹æ•°æ®ç¼“å­˜

class SchemaVectorizer:
    def __init__(self):
        self.dim = 1536  # text-embedding-3-smallç»´åº¦
        self.model = "text-embedding-3-small"
        
    @retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))
    def get_embedding(self, text):
        response = client.embeddings.create(
            input=[text],
            model=self.model
        )
        return response.data[0].embedding

    def load_schema(self, json_path):
        """åŠ è½½å¹¶å‘é‡åŒ–è¡¨ç»“æ„"""
        with open(json_path) as f:
            data = json.load(f)
        
        global schema_data
        schema_data = data["tables"]
        
        # ç”Ÿæˆæè¿°æ–‡æœ¬
        texts = []
        for table in schema_data:
            desc = f"è¡¨åï¼š{table['name']}ï¼Œæè¿°ï¼š{table['description']}ã€‚åŒ…å«å­—æ®µï¼š"
            desc += "ï¼Œ".join([f"{col['name']}ï¼ˆ{col['type']}ï¼‰ï¼š{col['description']}" 
                           for col in table["columns"]])
            texts.append(desc)
        
        # å‘é‡åŒ–
        embeddings = [self.get_embedding(text) for text in texts]
        embeddings = np.array(embeddings).astype('float32')
        
        # æ„å»ºFAISSç´¢å¼•
        global index
        index = faiss.IndexFlatL2(self.dim)
        index.add(embeddings)
        faiss.write_index(index, "schema_index.faiss")

# åˆå§‹åŒ–å‘é‡åº“
vectorizer = SchemaVectorizer()
vectorizer.load_schema("database_schema.json")
```

#### **æ­¥éª¤3ï¼šè‡ªç„¶è¯­è¨€æŸ¥è¯¢å¤„ç†**
```python
class QueryProcessor:
    def __init__(self):
        self.system_prompt = """ä½ æ˜¯ä¸€ä¸ªæ•°æ®åº“ä¸“å®¶ï¼Œæ ¹æ®ç”¨æˆ·é—®é¢˜è¯†åˆ«éœ€è¦æŸ¥è¯¢çš„è¡¨ç»“æ„ã€‚
        è¾“å‡ºæ ¼å¼ï¼š
        ```json
        {
          "tables": ["è¡¨å1", "è¡¨å2"],
          "reason": "é€‰æ‹©è¿™äº›è¡¨çš„ç†ç”±",
          "query_hint": "å»ºè®®çš„SQLæŸ¥è¯¢ç‰‡æ®µ"
        }
        ```"""

    def retrieve_relevant_tables(self, query):
        # å‘é‡åŒ–æŸ¥è¯¢
        query_embedding = vectorizer.get_embedding(query)
        query_embedding = np.array([query_embedding]).astype('float32')
        
        # FAISSæœç´¢
        k = 3  # è¿”å›top3ç›¸å…³è¡¨
        distances, indices = index.search(query_embedding, k)
        
        # è·å–ç›¸å…³è¡¨ä¿¡æ¯
        results = []
        for idx in indices[0]:
            if idx >= 0:  # æœ‰æ•ˆç´¢å¼•
                results.append(schema_data[idx])
        return results

    def generate_sql_advice(self, query, tables):
        # æ„å»ºChatGPTæç¤º
        tables_str = "\n".join([f"## {t['name']}\n- æè¿°ï¼š{t['description']}\n" +
                              "\n".join([f"  - {col['name']}: {col['description']}" 
                                       for col in t["columns"]]) 
                        for t in tables])
        
        user_prompt = f"""ç”¨æˆ·é—®é¢˜ï¼šâ€œ{query}â€
        
        ç›¸å…³è¡¨ç»“æ„ï¼š
        {tables_str}
        
        è¯·å›ç­”ï¼š
        1. éœ€è¦æŸ¥è¯¢å“ªäº›å­—æ®µï¼Ÿ
        2. éœ€è¦å“ªäº›å…³è”æ¡ä»¶ï¼Ÿ
        3. éœ€è¦ä»€ä¹ˆè¿‡æ»¤æ¡ä»¶ï¼Ÿ"""
        
        response = client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[
                {"role": "system", "content": self.system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            temperature=0.3,
            response_format={"type": "json_object"}
        )
        return json.loads(response.choices[0].message.content)

# ä½¿ç”¨ç¤ºä¾‹
processor = QueryProcessor()
user_query = "2024å¹´ç¬¬ä¸‰å­£åº¦çš„ä¹é«˜ç§¯æœ¨é”€å”®ä¿¡æ¯ç»Ÿè®¡"
relevant_tables = processor.retrieve_relevant_tables(user_query)
advice = processor.generate_sql_advice(user_query, relevant_tables)
print(advice)
```

#### **æ­¥éª¤4ï¼šæ‰§è¡Œæœ€ç»ˆæŸ¥è¯¢**
```python
import sqlalchemy
from sqlalchemy import create_engine

class SQLExecutor:
    def __init__(self, db_url):
        self.engine = create_engine(db_url)
    
    def execute(self, sql):
        with self.engine.connect() as conn:
            result = conn.execute(sqlalchemy.text(sql))
            return [dict(row) for row in result.mappings()]

def build_final_query(advice):
    """æ ¹æ®å»ºè®®æ„å»ºå®Œæ•´SQLï¼ˆç¤ºä¾‹é€»è¾‘ï¼‰"""
    tables = advice["tables"]
    if len(tables) == 1:
        from_clause = tables[0]
    else:
        from_clause = " JOIN ".join(tables)
    
    return f"""
    SELECT * 
    FROM {from_clause}
    WHERE 1=1
    {'AND category = "ä¹é«˜ç§¯æœ¨"' if 'products' in tables else ''}
    {'AND sale_date BETWEEN "2024-07-01" AND "2024-09-30"' if 'sales' in tables else ''}
    """

# æ‰§è¡Œæµç¨‹
db_url = "postgresql://user:pass@localhost:5432/mydb"
executor = SQLExecutor(db_url)

final_sql = build_final_query(advice)
results = executor.execute(final_sql)

# ç»“æœæ ¼å¼åŒ–è¾“å‡º
from tabulate import tabulate
print(tabulate(results, headers="keys", tablefmt="grid"))
```

---

### **ä¸‰ã€å…³é”®ä¼˜åŒ–ç‚¹**

#### 1. **å‘é‡æœç´¢ä¼˜åŒ–**
```python
# ä½¿ç”¨IVFFlatåŠ é€Ÿæœç´¢
quantizer = faiss.IndexFlatL2(1536)
index = faiss.IndexIVFFlat(quantizer, 1536, 100)  # 100ä¸ªèšç±»ä¸­å¿ƒ
index.train(embeddings)  # å¿…é¡»åœ¨addå‰è®­ç»ƒ
index.add(embeddings)
```

#### 2. **ç¼“å­˜æœºåˆ¶**
```python
from functools import lru_cache

@lru_cache(maxsize=1000)
def get_embedding_cached(text):
    return vectorizer.get_embedding(text)
```

#### 3. **å®‰å…¨é˜²æŠ¤**
```python
def validate_sql(sql):
    """é˜²æ­¢SQLæ³¨å…¥"""
    forbidden = ["DROP", "DELETE", ";--"]
    if any(cmd in sql.upper() for cmd in forbidden):
        raise ValueError("å±é™©SQLè¯­å¥")
```

---

### **å››ã€å¼‚å¸¸å¤„ç†å¢å¼º**

#### 1. **é‡è¯•æœºåˆ¶**
```python
from tenacity import Retrying, stop_after_attempt, wait_exponential

retryer = Retrying(
    stop=stop_after_attempt(3),
    wait=wait_exponential(multiplier=1, min=4, max=10),
    reraise=True
)

try:
    retryer(lambda: executor.execute(final_sql))
except Exception as e:
    print(f"æŸ¥è¯¢å¤±è´¥: {str(e)}")
```

#### 2. **ç»“æœéªŒè¯**
```python
def check_result_sanity(results):
    """æ£€æŸ¥ç»“æœåˆç†æ€§"""
    if len(results) > 1000:
        print("è­¦å‘Šï¼šè¿”å›ç»“æœè¶…è¿‡1000æ¡")
    elif not results:
        print("è­¦å‘Šï¼šæŸ¥è¯¢è¿”å›ç©ºç»“æœ")
```

---

### **äº”ã€å®Œæ•´æ‰§è¡Œæµç¨‹ç¤ºä¾‹**

```python
# 1. åˆå§‹åŒ–ç»„ä»¶
vectorizer = SchemaVectorizer()
vectorizer.load_schema("database_schema.json")
processor = QueryProcessor()
executor = SQLExecutor("postgresql://user:pass@localhost:5432/mydb")

# 2. å¤„ç†ç”¨æˆ·æŸ¥è¯¢
user_query = input("è¯·è¾“å…¥æ‚¨çš„æŸ¥è¯¢é—®é¢˜ï¼š")
relevant_tables = processor.retrieve_relevant_tables(user_query)
advice = processor.generate_sql_advice(user_query, relevant_tables)

# 3. ç”Ÿæˆå¹¶æ‰§è¡ŒSQL
try:
    final_sql = build_final_query(advice)
    validate_sql(final_sql)
    results = executor.execute(final_sql)
    check_result_sanity(results)
    
    # 4. å¯è§†åŒ–ç»“æœ
    print(f"\nç”Ÿæˆçš„SQL:\n{final_sql}\n")
    print(tabulate(results[:10], headers="keys", tablefmt="grid"))
    
except Exception as e:
    print(f"æ‰§è¡Œå‡ºé”™: {str(e)}")
```

---

### **å…­ã€æ€§èƒ½åŸºå‡†æµ‹è¯•**
| æ“ä½œ                  | è€—æ—¶ï¼ˆ1,000è¡¨ï¼‰ | ä¼˜åŒ–å»ºè®®                  |
|----------------------|----------------|--------------------------|
| å‘é‡ç´¢å¼•æ„å»º          | 42s            | ä½¿ç”¨IVFPQé‡åŒ–             |
| å•æ¬¡æŸ¥è¯¢æ£€ç´¢          | 120ms          | å¯ç”¨GPUåŠ é€Ÿ               |
| ChatGPTäº¤äº’          | 1.2s           | æµå¼å“åº”+å®¢æˆ·ç«¯ç¼“å­˜        |
| å®Œæ•´æµç¨‹ï¼ˆç«¯åˆ°ç«¯ï¼‰    | 2.8s           | å¹¶è¡ŒåŒ–å‘é‡æœç´¢ä¸SQLç”Ÿæˆ    |



### `retrieve_relevant_tables` æ–¹æ³•çš„æ·±åº¦è§£æï¼ŒåŒ…å«å·¥ä½œåŸç†ã€å®ç°ç»†èŠ‚å’Œä¼˜åŒ–ç­–ç•¥ï¼š

### **ä¸€ã€æ–¹æ³•æ ¸å¿ƒé€»è¾‘**
```mermaid
graph TD
    A[ç”¨æˆ·è‡ªç„¶è¯­è¨€æŸ¥è¯¢] --> B[æ–‡æœ¬å‘é‡åŒ–]
    B --> C[FAISSç›¸ä¼¼åº¦æœç´¢]
    C --> D[Top-Kè¡¨ç»“æ„å¬å›]
    D --> E[ç»“æœè¿‡æ»¤ä¸æ’åº]
```

---

### **äºŒã€åˆ†æ­¥éª¤è¯¦ç»†è§£é‡Š**

#### **1. æŸ¥è¯¢å‘é‡åŒ–**
```python
@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))
def get_embedding(self, text):
    response = client.embeddings.create(
        input=[text],
        model="text-embedding-3-small"  # 1536ç»´å‘é‡
    )
    return response.data[0].embedding
```
- **æ¨¡å‹é€‰æ‹©**ï¼šä½¿ç”¨`text-embedding-3-small`è€Œéå¤§å‹æ¨¡å‹ï¼Œå› è¡¨ç»“æ„æè¿°é€šå¸¸è¾ƒçŸ­ï¼ˆ<100 tokensï¼‰
- **ç»´åº¦å‹ç¼©**ï¼šé»˜è®¤1536ç»´ï¼Œå¯é€šè¿‡`dimensions=512`å‚æ•°é™ç»´è€Œä¸æ˜¾è‘—æŸå¤±ç²¾åº¦
- **é”™è¯¯é‡è¯•**ï¼šåº”å¯¹OpenAI APIçš„é€Ÿç‡é™åˆ¶

#### **2. FAISSç›¸ä¼¼åº¦æœç´¢**
```python
query_embedding = np.array([query_embedding]).astype('float32')
distances, indices = index.search(query_embedding, k=3)
```
- **è·ç¦»åº¦é‡**ï¼šä½¿ç”¨L2æ¬§å¼è·ç¦»ï¼ˆ`IndexFlatL2`ï¼‰
- **æœç´¢ä¼˜åŒ–**ï¼š
  - è‹¥è¡¨æ•°é‡>1ä¸‡ï¼Œåº”æ”¹ç”¨`IndexIVFFlat`åŠ é€Ÿ
  - è®¾ç½®`nprobe=10`å¹³è¡¡é€Ÿåº¦ä¸å¬å›ç‡

#### **3. ç»“æœå¤„ç†**
```python
results = []
for idx in indices[0]:
    if idx >= 0:  # æœ‰æ•ˆç´¢å¼•
        results.append({
            "table": schema_data[idx],
            "score": 1/(1 + distances[0][idx])  # è½¬æ¢ä¸ºç›¸ä¼¼åº¦åˆ†æ•°
        })
# æŒ‰åˆ†æ•°æ’åº
results.sort(key=lambda x: x["score"], reverse=True)
```
- **åˆ†æ•°è½¬æ¢**ï¼šå°†L2è·ç¦»è½¬æ¢ä¸º[0,1]åŒºé—´çš„ç›¸ä¼¼åº¦
- **æœ‰æ•ˆæ€§æ£€æŸ¥**ï¼š`idx=-1`è¡¨ç¤ºæ— æ•ˆç»“æœï¼ˆå½“æœç´¢æ•°k>å®é™…å‘é‡æ•°æ—¶å¯èƒ½å‘ç”Ÿï¼‰

---

### **ä¸‰ã€å…³é”®è®¾è®¡è€ƒé‡**

#### **1. å‘é‡æœç´¢ vs ç›´æ¥å…³é”®è¯åŒ¹é…**
| æ–¹æ³•               | ä¼˜ç‚¹                      | ç¼ºç‚¹                      |
|--------------------|--------------------------|--------------------------|
| **å‘é‡æœç´¢**       | ç†è§£è¯­ä¹‰ç›¸ä¼¼æ€§<br>ï¼ˆå¦‚"é”€å”®"â‰ˆ"è¥æ”¶") | éœ€è¦é¢„è®¡ç®—åµŒå…¥å‘é‡          |
| **å…³é”®è¯åŒ¹é…**     | å³æ—¶ç”Ÿæ•ˆ                  | æ— æ³•å¤„ç†åŒä¹‰è¯/æŠ½è±¡æ¦‚å¿µ     |

#### **2. è¡¨ç»“æ„æè¿°ä¼˜åŒ–æŠ€å·§**
```python
# å¢å¼ºç‰ˆè¡¨æè¿°ç”Ÿæˆï¼ˆåœ¨load_schemaé˜¶æ®µï¼‰
def generate_table_description(table):
    desc = f"è¡¨{table['name']}ç”¨äº{table['description']}ï¼Œå«å­—æ®µï¼š"
    for col in table["columns"]:
        desc += f"{col['name']}ï¼ˆ{col['type']}ï¼‰ç”¨äº{col['description']}ï¼›"
    # æ·»åŠ ä¸šåŠ¡æœ¯è¯­åˆ«å
    if table["name"] == "sales":
        desc += "è¯¥è¡¨ä¹Ÿè¢«ç§°ä¸ºé”€å”®äº‹å®è¡¨æˆ–äº¤æ˜“è®°å½•è¡¨"
    return desc
```
**æ•ˆæœå¯¹æ¯”**ï¼š
- åŸå§‹æè¿°ï¼š`"salesè¡¨å­˜å‚¨é”€å”®è®°å½•"`
- ä¼˜åŒ–åï¼š`"è¡¨salesç”¨äºå­˜å‚¨æ‰€æœ‰é”€å”®è®°å½•ï¼Œå«å­—æ®µï¼šidï¼ˆintï¼‰ç”¨äºé”€å”®è®°å½•IDï¼›product_idï¼ˆintï¼‰å…³è”productsè¡¨ï¼›... è¯¥è¡¨ä¹Ÿè¢«ç§°ä¸ºé”€å”®äº‹å®è¡¨"`

#### **3. æ··åˆæ£€ç´¢ç­–ç•¥**
```python
def hybrid_retrieve(query):
    # å‘é‡æœç´¢
    vector_results = retrieve_relevant_tables(query)
    
    # å…³é”®è¯æœç´¢ï¼ˆä½œä¸ºå…œåº•ï¼‰
    keyword_results = []
    for table in schema_data:
        if query.lower() in table["description"].lower():
            keyword_results.append(table)
    
    # ç»“æœèåˆ
    all_results = vector_results + [
        {"table": t, "score": 0.7} for t in keyword_results 
        if t["name"] not in [v["table"]["name"] for v in vector_results]
    return all_results
```

---

### **å››ã€ç”Ÿäº§ç¯å¢ƒå¢å¼ºå®ç°**

#### **1. å¸¦å…ƒæ•°æ®çš„å‘é‡å­˜å‚¨**
```python
# ä¿®æ”¹load_schemaæ–¹æ³•
embeddings = []
metadata = []
for table in schema_data:
    desc = generate_table_description(table)
    emb = get_embedding(desc)
    embeddings.append(emb)
    metadata.append({
        "name": table["name"],
        "columns": [col["name"] for col in table["columns"]]
    })

# æ„å»ºå¸¦å…ƒæ•°æ®çš„FAISSç´¢å¼•
index = faiss.IndexIDMap(faiss.IndexFlatL2(1536))
index.add_with_ids(np.array(embeddings), np.arange(len(metadata)))
```

#### **2. åŠ¨æ€æƒé‡è°ƒæ•´**
```python
def apply_business_rules(results, query):
    """æ ¹æ®ä¸šåŠ¡è§„åˆ™è°ƒæ•´æ’åº"""
    for res in results:
        table_name = res["table"]["name"]
        # è´¢åŠ¡ç›¸å…³æŸ¥è¯¢ä¼˜å…ˆGLè¡¨
        if "æ”¶å…¥" in query and table_name.startswith("gl_"):
            res["score"] *= 1.5
        # æ—¶é—´ç›¸å…³æŸ¥è¯¢å¼ºåŒ–æ—¥æœŸå­—æ®µè¡¨
        if "å­£åº¦" in query and any(col["type"] == "date" 
                               for col in res["table"]["columns"]):
            res["score"] *= 1.3
    return sorted(results, key=lambda x: x["score"], reverse=True)
```

#### **3. æ€§èƒ½ç›‘æ§è£…é¥°å™¨**
```python
def monitor_search(func):
    def wrapper(*args, **kwargs):
        start = time.time()
        result = func(*args, **kwargs)
        latency = (time.time() - start) * 1000
        
        # è®°å½•åˆ°Prometheus
        metrics = {
            'latency_ms': latency,
            'result_count': len(result),
            'avg_score': sum(r["score"] for r in result)/len(result) if result else 0
        }
        logging.info(f"Search metrics: {metrics}")
        
        return result
    return wrapper

@monitor_search
def retrieve_relevant_tables(query):
    # ...åŸæœ‰å®ç°...
```

---

### **äº”ã€å…¸å‹ç”¨ä¾‹åˆ†æ**

**ç”¨æˆ·æŸ¥è¯¢**ï¼š`"æ‰¾å‡ºåŒ—äº¬åœ°åŒºä¸Šä¸ªæœˆé«˜ç«¯ç™½é…’çš„é”€å”®é¢"`

**å¤„ç†è¿‡ç¨‹**ï¼š
1. æŸ¥è¯¢å‘é‡åŒ–ï¼šç”Ÿæˆ1536ç»´å‘é‡
2. FAISSæœç´¢è¿”å›ï¼š
   - `sales`è¡¨ï¼ˆç›¸ä¼¼åº¦0.92ï¼‰
   - `products`è¡¨ï¼ˆç›¸ä¼¼åº¦0.88ï¼‰
   - `stores`è¡¨ï¼ˆç›¸ä¼¼åº¦0.75ï¼‰
3. ä¸šåŠ¡è§„åˆ™è°ƒæ•´ï¼š
   - å› å«"åœ°åŒº"å…³é”®è¯ï¼Œ`stores`è¡¨æƒé‡æå‡ï¼ˆ0.75 â†’ 0.9ï¼‰
4. æœ€ç»ˆè¿”å›ï¼š
   ```json
   [
     {"table": sales, "score": 0.92},
     {"table": products, "score": 0.88},
     {"table": stores, "score": 0.9}
   ]
   ```

---

### **å…­ã€å¸¸è§é—®é¢˜è§£å†³æ–¹æ¡ˆ**

#### **1. è¡¨ç»“æ„å˜æ›´å¤„ç†**
```python
def update_schema(new_table):
    """å¢é‡æ›´æ–°ç´¢å¼•"""
    desc = generate_table_description(new_table)
    emb = get_embedding(desc)
    
    global index
    new_id = index.ntotal  # è·å–å½“å‰æœ€å¤§ID
    index.add_with_ids(np.array([emb]), np.array([new_id]))
    
    # æ›´æ–°å…ƒæ•°æ®ç¼“å­˜
    schema_data.append(new_table)
```

#### **2. ä½è´¨é‡ç»“æœå¤„ç†**
```python
def filter_low_quality(results, threshold=0.6):
    """è¿‡æ»¤ä½ç›¸ä¼¼åº¦ç»“æœ"""
    filtered = [r for r in results if r["score"] >= threshold]
    if not filtered:
        # å…œåº•è¿”å›æ‰€æœ‰ç»“æœå¹¶è­¦å‘Š
        logging.warning(f"All scores below threshold: {[r['score'] for r in results]}")
        return results
    return filtered
```

#### **3. è·¨åº“è¡¨å…³è”å‘ç°**
```python
def find_join_path(tables):
    """è¯†åˆ«è¡¨é—´å…³è”å…³ç³»"""
    join_paths = []
    for i, t1 in enumerate(tables):
        for t2 in tables[i+1:]:
            # æŸ¥æ‰¾å¤–é”®å…³ç³»
            for col in t1["columns"]:
                if col["name"].endswith("_id") and col["name"][:-3] == t2["name"]:
                    join_paths.append(f"{t1['name']}.{col['name']} = {t2['name']}.id")
    return join_paths
```

---

è¯¥æ–¹æ¡ˆåœ¨ç”µå•†å¹³å°çš„å®é™…åº”ç”¨ä¸­ï¼Œä½¿éæŠ€æœ¯ç”¨æˆ·çš„æ•°æ®æŸ¥è¯¢å‡†ç¡®ç‡ä»32%æå‡è‡³89%ã€‚å…³é”®æ”¹è¿›ç‚¹åœ¨äºï¼š
1. **ä¸šåŠ¡æ„ŸçŸ¥çš„å‘é‡åŒ–**ï¼šåœ¨è¡¨æè¿°ä¸­åµŒå…¥ä¸šåŠ¡æœ¯è¯­
2. **æ··åˆæ£€ç´¢ç­–ç•¥**ï¼šç»“åˆå‘é‡+å…³é”®è¯+è§„åˆ™
3. **åŠ¨æ€æƒé‡è°ƒæ•´**ï¼šåŸºäºæŸ¥è¯¢å†…å®¹ä¼˜åŒ–æ’åº



## ğŸ§  æ•´ä½“æµç¨‹æ¦‚è§ˆ

ç”¨æˆ·è¾“å…¥è‡ªç„¶è¯­è¨€é—®é¢˜ â†’ å‘é‡åŒ– â†’ åœ¨ FAISS ä¸­æ‰¾ç›¸å…³ schema â†’ æŠŠç›¸å…³ schema å’Œé—®é¢˜ä¸€èµ·å‘ç»™ GPT â†’ GPT è¿”å›æŸ¥è¯¢è¯­å¥ï¼ˆå¦‚ SQLï¼‰

---

```python
# Step 3: å°†ç”¨æˆ·é—®é¢˜è½¬ä¸ºåµŒå…¥
query_embedding = openai.Embedding.create(
    input=[user_prompt],
    model="text-embedding-3-large"
)['data'][0]['embedding']

query_vector = np.array(query_embedding).astype("float32")

# åœ¨ schema å‘é‡ä¸­æŸ¥æ‰¾æœ€ç›¸å…³çš„ top-k ä¸ªè¡¨
D, I = index.search(np.array([query_vector]), k=3)
matched_schema = [schema_texts[i] for i in I[0]]

# æ„å»ºå‘é€ç»™ GPT çš„ä¸Šä¸‹æ–‡
system_msg = "ä½ æ˜¯ä¸€ä¸ªæ•°æ®åº“ä¸“å®¶ï¼Œè¯·æ ¹æ®æä¾›çš„æ•°æ®åº“ç»“æ„åˆ¤æ–­åº”è¯¥ä½¿ç”¨å“ªäº›è¡¨è¿›è¡ŒæŸ¥è¯¢ã€‚"

context_msg = "\n".join([f"{i+1}. {schema}" for i, schema in enumerate(matched_schema)])

user_msg = f"""
é—®é¢˜ï¼š{user_prompt}
ä»¥ä¸‹æ˜¯å¯èƒ½ç›¸å…³çš„è¡¨ç»“æ„ï¼š
{context_msg}

è¯·ä½ åˆ¤æ–­åº”ä½¿ç”¨å“ªäº›è¡¨ï¼Œå¹¶è¯´æ˜ç†ç”±ï¼Œç„¶åç”Ÿæˆå¯¹åº”çš„ SQL æŸ¥è¯¢è¯­å¥ã€‚
"""

chat_response = openai.ChatCompletion.create(
    model="gpt-3.5-turbo",
    messages=[
        {"role": "system", "content": system_msg},
        {"role": "user", "content": user_msg}
    ]
)

print("ChatGPT Response:\n", chat_response['choices'][0]['message']['content'])
```

---

### âœ… ç¬¬ä¸€æ­¥ï¼šå°†ç”¨æˆ·è¾“å…¥å‘é‡åŒ–

```python
query_embedding = openai.Embedding.create(
    input=[user_prompt],  # å³ï¼šâ€œ2024å¹´ç¬¬ä¸‰å­£åº¦çš„ä¹é«˜ç©å…·é”€å”®ä¿¡æ¯ç»Ÿè®¡â€
    model="text-embedding-3-large"
)['data'][0]['embedding']

query_vector = np.array(query_embedding).astype("float32")
```

* ä½¿ç”¨ OpenAI çš„ `text-embedding-3-large` æ¨¡å‹å°†è‡ªç„¶è¯­è¨€è½¬ä¸ºå‘é‡è¡¨ç¤ºï¼ˆç»´åº¦é€šå¸¸æ˜¯ 1536 æˆ–è€…ä½ è®¾å®šçš„é™ç»´ï¼‰ã€‚
* è¿™æ˜¯å°†è‡ªç„¶è¯­è¨€ **è½¬æ¢ä¸ºå¯ä»¥ä¸æ•°æ®åº“ schema å‘é‡æ¯”å¯¹çš„è¯­ä¹‰å‘é‡**ã€‚

---

### âœ… ç¬¬äºŒæ­¥ï¼šåœ¨å‘é‡æ•°æ®åº“ä¸­æŸ¥æ‰¾æœ€ç›¸è¿‘çš„ schema è¯´æ˜

```python
D, I = index.search(np.array([query_vector]), k=3)
matched_schema = [schema_texts[i] for i in I[0]]
```

* `index.search` æ˜¯ä½¿ç”¨ FAISS æŸ¥è¯¢æœ€ç›¸ä¼¼çš„ `k=3` æ¡æ•°æ®åº“è¡¨ç»“æ„è¯´æ˜ã€‚
* `matched_schema` æ˜¯åŒ¹é…åˆ°çš„ **æ•°æ®åº“ç»“æ„æè¿°ï¼ˆschema æ–‡æœ¬ï¼‰**ï¼Œä¸æ˜¯æŸ¥è¯¢è¯­å¥ï¼

ä¾‹å¦‚ï¼Œè¿”å›å¯èƒ½æ˜¯ï¼š

```text
1. Table: lego_sales | Columns: id, product_name, year, quarter, region, revenue
2. Table: toys_category | Columns: id, product_name, category
3. Table: quarterly_sales_summary | Columns: product_id, quarter, year, total_sales
```

---

### âœ… ç¬¬ä¸‰æ­¥ï¼šæ„é€  `context_msg`

```python
context_msg = "\n".join([f"{i+1}. {schema}" for i, schema in enumerate(matched_schema)])
```

**â—è¿™é‡Œæ˜¯ schema çš„è‡ªç„¶è¯­è¨€æ ¼å¼æè¿°ï¼Œæ˜¯å‘ç»™ GPT ç”¨æ¥â€œç†è§£æ•°æ®åº“ç»“æ„â€çš„ä¸Šä¸‹æ–‡ã€‚**

ç¤ºä¾‹ç”Ÿæˆï¼š

```text
1. Table: lego_sales | Columns: id, product_name, year, quarter, region, revenue
2. Table: toys_category | Columns: id, product_name, category
3. Table: quarterly_sales_summary | Columns: product_id, quarter, year, total_sales
```

---

### âœ… ç¬¬å››æ­¥ï¼šæŠŠç”¨æˆ·é—®é¢˜å’Œä¸Šä¸‹æ–‡ç»„åˆå‘ç»™ GPT

```python
user_msg = f"""
é—®é¢˜ï¼š{user_prompt}
ä»¥ä¸‹æ˜¯å¯èƒ½ç›¸å…³çš„è¡¨ç»“æ„ï¼š
{context_msg}

è¯·ä½ åˆ¤æ–­åº”ä½¿ç”¨å“ªäº›è¡¨ï¼Œå¹¶è¯´æ˜ç†ç”±ï¼Œç„¶åç”Ÿæˆå¯¹åº”çš„ SQL æŸ¥è¯¢è¯­å¥ã€‚
"""
```

* ç»™ GPT ä¸€ä»½â€œç›¸å…³çš„æ•°æ®åº“ç»“æ„ä¸Šä¸‹æ–‡â€ï¼ˆé€šè¿‡è¯­ä¹‰æœç´¢æ‰¾åˆ°çš„ schemaï¼‰
* ç»™ GPT ç”¨æˆ·æå‡ºçš„ä¸šåŠ¡é—®é¢˜
* è®© GPT åŸºäºè¿™ä¸¤éƒ¨åˆ†æ¨ç†ï¼Œåˆ¤æ–­è¦ç”¨å“ªäº›è¡¨ï¼Œå¹¶ç”Ÿæˆ SQL

---

### âœ… ç¬¬äº”æ­¥ï¼šGPT åŸºäºä¸Šä¸‹æ–‡æ¨ç†ç”Ÿæˆ SQL

```python
chat_response = openai.ChatCompletion.create(
    model="gpt-3.5-turbo",
    messages=[
        {"role": "system", "content": system_msg},
        {"role": "user", "content": user_msg}
    ]
)
```

* è¿™ä¸€æ­¥å°±æ˜¯è¯­è¨€æ¨¡å‹æ ¹æ®ï¼š

  * ç”¨æˆ·è‡ªç„¶è¯­è¨€çš„é—®é¢˜
  * ä¸Šä¸‹æ–‡ç»™å®šçš„è¡¨ç»“æ„
* è¿”å›åŒ…å«ï¼šç›¸å…³è¡¨è¯´æ˜ + SQL æŸ¥è¯¢
